---
title: "Boite à outils R"
author: "T. VENET"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               eval = FALSE,
               results = "hide")
opts_knit$set(width=75)
```

# PACKAGES UTILES

[***A copier à chaque début de script !***]{.ul}

```{r}
library(tidyverse)
library(questionr)
library(gtsummary)
theme_gtsummary_language("fr", decimal.mark = ",", big.mark = " ") # Option de mise en forme des tableaux gtsummary
options(scipen = 999) # Pour désactiver l'écriture scientifique ( = 0 pour réactiver)
```

# **IMPORTATION DE DONNEES**

<https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-import.pdf>

C'est pas mal d'indiquer un espace travail avant d'importer les données.

-   Soit en créant un projet R (File \> New project ...)

-   Soit avec Files \> More \> Set As Workind Directory

-   Soit avec :

```{r}
setwd("chemin/du/dossier")
```

## Import d'un fichier .csv (formule de base)

```{r}
NOM <- read.csv2("fichier.csv")
```

## Import d'un fichier .csv avec des noms de lignes

```{r}
NOM <- read.csv2("fichier.csv", row.names=1)
```

## Import d'un fichier Excel

```{r1}
library(readxl)
NOM <- read_excel("chemin d'adresse/Nomdufichier.xlsx")
```

## Import de fichiers se trouvant dans plusieurs feuilles d'un fichier excel

```{r1}
library(readxl)
NOM1 <- read_excel("fichier.xlsx", sheet = "feuille 1")
NOM2 <- read_excel("fichier.xlsx", sheet = "feuille 2")
NOM... <- read_excel("fichier.xlsx", sheet = "feuille ...")
NOMn <- read_excel("fichier.xlsx", sheet = "feuille n")
```

# **ANALYSES EXPLORATOIRES**

DataExplorer : package interessant ! -> https://delladata.fr/analyses-exploratoires-package-dataexplorer/

Avec notamment la possibilité de faire un graph des non réponses :

```{r}
library(DataExplorer)
t <- titanic::titanic_train
plot_missing(t) 
```

`gtextra` permet aussi des choses trés sympas, notamment avec `gt_plt_summary()`

```{r echo=TRUE}
library(gtExtras)
gt_plt_summary(t) 
```


# **MANIPULATION DES DONNEES**

<https://raw.githubusercontent.com/rstudio/cheatsheets/master/translations/french/data-wrangling_fr.pdf>

## Ajout de labels - étiquettes sur les variables

```{r}
library(labelled)
var_label(DATA$VAR) <- "texte étiquette de variable"
```

## Convertir une variable *character* en *facteur*

```{r}
DATA$VAR <- as.factor(DATA$VAR)
```

## Renommer des variables

```{r}
DATA <- DATA %>% rename(NouveauNomVar1 = AncienNomVar1, NouveauNomVar2 = AncienNomVar2, ... ,NouveauNomVarn = AncienNomVarn)
```

## Recoder des modalités

### Renommer et regrouper des modalités

```{r}
DATA$VAR_rec <- as.character(DATA$VAR)
DATA$VAR_rec <- fct_recode(DATA$VAR_rec,
  "NouvNomMod1" = "AncNomMod1",
  "NouvNomMod2" = "AncNomMod2",
  ... = ...,
  "NouvNomModn" = "AncNomModn")
```

### Version interactive avec *irec* de *questionr*

```{r}
irec(data, var)
```

### A la main, ne tenant compte des modalités d'autres variable

```{r}
Data$VarRecod <- Data$VarInitiale
Data$VarRecod[Data$VarTri == "Modalite"] <- Modalité1
...
```


## Discretiser une variable quantitative

### Discrétisation automatiques

```{r}
data$VarClasse <- cut(data$VarNum, n)
```

où n est le nombre de classes voulu (par défaut, la fonction fait des classes d'amplitudes égales)

### Dicrétisation manuelle

```{r}
data$VarClasse <- cut(data$VarNum, c(b1, b2, b3, ..., bn))
```

où b1, b2, b3, ..., bn sont les bornes des classes voulues

### Mode interactif avec *icut* de *questionr*

```{r}
icut(data, VarNum)
```

## Changer la modalité de reférence d'un facteur

```{r}
DATA$VAR <- relevel(DATA$VAR, ref= "Modalité")
```

## Calculer une nouvelle variable

```{r}
data <- mutate(data, NouvVar = calcule)
```

## Juxtaposer des tableaux de données (trés dangereux)

```{r}
cbind(tab1, tab2)
```

## Superposer des tableaux de données (trés dangereux)

```{r}
rbind(tab1, tab2)
```

A éviter si les lignes ne sont pas le même ordre... A la rigueur, il vaut mieux utiliser :

```{r}
bind_rows(tab1, tab2, tab3)
```

mais ça ne marchera que si les colonnes ont les mêmes noms (mais pas forcément dans le même ordre)

## Fusionner deux bases de données en utilisant des identifiants de lignes identiques (à privilégier)

Quand les 2 tableaux ont des identifiants dans des colonnes de mêmes noms (par ex. "id"). On utilise les colonnes "id" des deux fichiers pour faire la jointure. *C'est l'équivalent de =RechercheV(...) sous Excel*

```{r}
merge(tab1, tab2, by = "id")
```

Par défaut, merge ne garde [que les individus qui sont présents dans les 2]{.ul} bases initiales. On peut modifier ça avec les options all.x (pour concerver toutes les lignes du premier tab) et all.y (pour concerver toutes les lignes du second tab).

```{r}
merge(tab1, tab2, by = "id", all.x = TRUE, all.y = TRUE)
```

Avec dplyr, il y a plusieurs formules, qui correspondent pour certaines aux différentes options vues ci-dessus :

-   left_join fait une jointure à gauche, équivelent de l'option all.x de merge

-   right_join fait une jointure à droite, équ. de l'opt. all.y de merge

> (On a une symétrie entre ces fonctions : left_join(x,y) = right_join(y,x) )

-   inner_join rassemble les individus qui sont présents dans les deux fichier

-   full_join rassemble tous les individus

```{r}
full_join(tab1, tab2, by = "id")
```

Petite variante si les variables identifiantes n'ont pas les mêmes noms :

```{r}
left_join(tab1, tab2, by = c("id1" = "id2"))
```

-   semi_join et anti_join sont des fonctions filtrantes

Aprés la jonction, c'est pas mal de faire une extraction de la nouvelle base pour vérifier les lignes sous excel. (voir section [Exporter une base de données](https://thovenet.github.io/#exporter-une-base-de-donn%C3%A9es))

## Extraire une sous-population (selectionner certaines observations/lignes/individus)

### A partir de la position de la ligne dans le fichier

#### Fonctions de base R

```{r}
DATA[1, ]
```

Sort la ligne correspondant à l'individu 1 du fichier DATA

```{r}
DATA[1:45, ]
```

Sort les lignes correspondants aux individus 1, 2, 3, ... et 45 du fichier DATA

```{r}
DATA[c(10,12,14), ]
```

Sort les lignes 10, 12 et 14

#### Avec dplyr

Même chose avec la grammaire dplyr (fonction slice)

```{r}
slice(DATA, 1)
slice(DATA, 1:45)
slice(DATA, 10, 12, 14) # Ecriture plus souple que slice(data, c(10,12,14)) qui marche aussi
```

Pour faciliter la recherche des individus, on peut faire un objet avec le facteur de position, et faire ensuite l'extraction des individus en utilisant cette objet position

```{r}
position <- c(10, 12, 14)
slice(data,position)
```

### A partir d'une condition (critères)

#### Base R

```{r}
data[data$var == "modalité", ]
```

Filtre la population pour ne garder que la sous-population qui a le critère souhaité

```{r}
data[data$var =! "modalité", ]
```

Filtre la population en excluant les individus qui ont une caractéristique non-souhaitée

#### Avec dplyr

```{r}
filter(data, var == "modalité")
```

Si on a une variable numérique, on peut facilement filtrer selon un intervalle (de 5 à 15 par ex.) :

```{r}
filter(data, varnum >= 5 & varnum <= 15 )
```

D'une manière générale, pour cumuler les critères on utilise & et \| :

& permet de faire un "et" conditionnel

| (alt.gr + 6) permet de faire un "ou" conditionnel

Si on oublie le symbole entre les conditions, filter appliquera un "et" par défaut

On peut même fonder le critère de tri sur un calcul (par exemple la valeur maximale) :

```{r}
filter(data, varnum == max(varnum))
```

Dans ?filter, on trouve toute sorte d'options utiles pour le tris

## Exporter une base de données

```{r}
write_csv2(DATA, file = "monFichier.csv")
```

# **ANALYSES UNIVARIEES**

<https://rstudio-pubs-static.s3.amazonaws.com/224337_f0de438bd82e4a769e55e039e33b6a0a.html>

## Une variable numérique

```{r}
summary(DATA$VARNUM)
```

Donne les valeurs minimum et maximum , les 1er et 3eme quartiles, la médiane, la moyenne et le nombre de non-réponses

### Tendances centrales

#### Moyenne

```{r}
mean(DATA$VAR, na.rm = TRUE)
```

#### Médiane

```{r}
median(DATA$VAR, na.rm = TRUE)
```

### Dispersion

#### Calculer des quantiles

```{r}
quantile(DATA$VAR, na.rm = TRUE)
```

Par défaut, la commande sort les min, max, médiane et quartiles. On peut demander des quantiles spécifiques en ajoutant l'option probs = 0:5/5 (pour couper la pop en 5) par exemple, ou manuellement probs = c(x,y,z,...,n)

#### Minimum / Maximum

```{r}
min(DATA$VAR, na.rm = TRUE)
```

```{r}
max(DATA$VAR, na.rm = TRUE)
```

```{r}
range(DATA$VAR, na.rm = TRUE)
```

#### Variance

```{r}
var(DATA$VAR, na.rm = TRUE)
```

#### Ecart-type

```{r}
sd(DATA$VAR, na.rm = TRUE)
```

## Une variable catégorielle

### Tri à plat

```{r}
summary(DATA$VarCat)
```

```{r}
table(DATA$VARIABLE)
```

La même chose, en écriture *formule*

```{r}
xtabs(~ VARIABLE, data=DATA)
```

### Tableau avec questionr

```{r}
freq(DATA$VARIABLE, total =T) 
```

Plus pratique : sort directement les effectifs et les pourcentages, on peut demander des effectifs cumulés avec *cum = True*

### Tableau avec gt_summary

```{r}
tbl_summary(DATA, include = c("VARIABLE1", "VARIABLE2", "VARIABLEx"))
```

Tableau encore plus joli, qui tient compte des etiquettes des variables. Include =c(...) permet de préciser les variables à décrire (sans précision, toutes les variables seront affichées). Le tableau sort les médianes et quartiles des variables numériques.

# **ANALYSES BIVARIEES**

<https://rstudio-pubs-static.s3.amazonaws.com/224337_f0de438bd82e4a769e55e039e33b6a0a.html>

## Deux variables qualitatives

### Tableau croisé

#### Tableau de contingence

```{r}
tab <- xtabs(Varpoids(optionnelle) ~ Var1 + Var2, data = données)
```

#### Fréquences en ligne

```{r}
rprop(tab)
```

#### Fréquences en colonne

```{r}
cprop(tab)
```

#### Fréquences au total

### Test de khi2

```{r}
chisq.test(tab)
```

### Avec gt_summary

```{r}
données %>%
  tbl_summary(
    include = c("Var1","Var2",...),
    by = "Var3"
  ) %>%
  add_p()

```
Possibilité de personnaliser presque tous les elements du tableau. 
gt_summary évolue en permanence.


## Une variable qualitative et une variable quantitative

On peut calculer des moyennes par groupes avec :

```{r}
tapply(DATA$VarQuanti, DATA$VarQuali, mean)
```

(mais les Non réponses sont mal-gérées)

Autre possibilité, qui en plus donne les intervalles de confiance !

```{r}
library(Publish)
ci.mean(VarNum ~ VarQuali, DATA)
```

### Une variable quantitative et une variable dichotomique

***t-test si les variances sont égales et si les distributions suivent des lois normales :***

```{r}
t.test(VarNum ~ VarDichotomique, data = DATA)
```

Pour vérifier la normalité :

```{r}
#Compare les histogrammes
par(mfrow = c(1, 2))
hist(DATA$VarNum[DATA$VarDich == "Modalité1"], main = "Modalité2", col = "red")
hist(DATA$VarNum[DATA$VarDich == "Modalité2"], main = "Modalité2", col = "red")
```

Test de normalité :

```{r}
shapiro.test(DATA$VarNum[DATA$VarDich == "Modalité1"])
shapiro.test(DATA$VarNum[DATA$VarDich == "Modalité2"])
```

Pour comparer les variances :

```{r}
tapply(DATA$VarNum, DATA$VarDich, var)
var.test(VarNum ~ VarDich, data = DATA)
```

***test de wilcoxon/Mann-Whitney si ce n'est pas le cas (test non paramètrique sur les rangs)***

```{r}
wilcox.test(VarNum ~ VarDich, data = DATA)
```

### gt_summary, c'est une *révolution*

<http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html>

On peut croiser plusieurs variables numériques avec un facteurs à plusieurs niveaux, et demander une p-value permettant d'évaluer la significativité des écarts.

```{r}
DATA %>% tbl_summary(include = c("VarNum1", "VarNum2", ... , "VarNumN"), by = VarQuali) %>% add_p()
```

On peut même demander des stats particulières dans le tableau avec l'option statistic = ... Par exemple, la syntaxe suivante demande les moyennes et écarts types pour les variables quanti (par defaut c'est médiane et quartiles)

```{r}
DATA %>% tbl_summary(include = c("VarNum1", "VarNum2", ... , "VarNumN"), by = VarQuali, statistic = all_continuous()~"{mean} ({sd})") %>% add_p()

```

En ajoutant l'option missing = "no", on masque l'info sur les réponses manquantes

```{r}
DATA %>% tbl_summary(include = c("VarNum1", "VarNum2", ... , "VarNumN"), by = VarQuali, statistic = all_continuous()~"{mean} ({sd})", missing = "no") %>% add_p()
```

## Deux variables quantitatives

<https://delladata.fr/la-regression-lineaire-simple-avec-le-logiciel-r/>

\#\#\#Visualiser le lien entre les variables :

```{r}
library(car)
scatterplot(VarNum1~VarNum2, data=DATA)
```

Affiche le nuage de point, les boxplots des deux distributions, la droite de régression linéaire par la methode des MCO (en ligne continue) et la courbe de regression locale de type lowess (en pointillés ) et son intervalle de confiance.

> Bonus sur la regression de Lowess : <http://perso.ens-lyon.fr/lise.vaudor/regression-loess/>

\#\#\#Construire la regression :

```{r}
reg <- lm(VarNum1~VarNum2, data=DATA)
```

\#\#\#Evaluer la validité des hypothèses

On vérifie que les résidus sont indépendants ; distribués suivant une loi normale ; avec une variance constante.

Indépendance

```{r}
acf(residuals(reg), main="reg") 
```

Normalité

```{r}
plot(reg,2)
```

Homogénéité

```{r}
plot(reg,3)
```

### Visualiser les résultats

```{r}
summary(reg)
```

La partie `Residuals` des résultats permet d'évaluer rapidement la normalité des résidus. Lorsque les résidus sont distribués selon une loi Normale, la médiane doit être autour de 0, et les valeurs absolues de Q1 et Q3 doivent être proches.

La première ligne de la partie coefficients concerne l'ordonnée à l'origine, la seconde ligne concerne la pente.

la première colonne rapporte l'estimation des coefficients des paramètres, la seconde colonne l'estimation de leur erreur standard, la troisième colonne est la statistique T, la dernière colonne rapporte la p-value du test évaluant l'égalité à 0 des coefficients.

En général, seul le coefficient de la pente a vraiment un intérêt. Cela signifie que lorsque la VarNum1 augmente d'une unité, alors, la VarnNum2 augmente de x unités (x étant la valeur de la pente).

```{r}
confInt(reg)
```

Affiche l'intervalle de confiance à 95% de la pente.

### Faire des prédications avec le modele

On commence par construire un nouveau data.frame avec une variable portant le même nom que la VarNum1 (ou 2) et les valeurs souhaitées.

```{r}
my_df <- data.frame(VarNum1 = c(Val1, Val2, ..., Valn))
```

Ce data.frame sert d'argument "newdata" de la fonction "predict"

```{r}
predict(reg, newdata=my_df)
```

On peut demander l'intervalle de confiances à 95% (la plage de valeurs ayant une probabilité de 0.95 de contenir la vraie valeur, pour le niveau utilisé)

```{r}
predict(reg, newdata=my_df, interval="confidence")
```

# **GRAPHIQUES ET REPRESENTATIONS VISUELLES**

-   [https://fxjollois.github.io/cours-2016-2017/visualisation-donnees.html\#avec_r-base\_(package_graphics)](https://fxjollois.github.io/cours-2016-2017/visualisation-donnees.html#avec_r-base_(package_graphics))

-   

## Affichage multiple de graphiques pour visualiser les données

```{r}
library(GGally)
ggpairs(DATA[, c("VAR1", "VAR2", "VAR3", "VAR4", ... , "VARn")])
```

## Générer un graphique avec *esquisse*

Outils *esquisse* pour générer des graphiques de manière interactive

```{r}
esquisse:esquisser()
```

On se sert de cet outils pour générer le code permettant de faire le graphique sous ggplot2. On peut ensuite modifier le code pour ajouter des options / enrichir le graphique.

## Options supplémentaires pour de jolis graphiques

[\<https://github.com/rstudio/cheatsheets/raw/master/translations/french/data-visualization_fr.pdf\>](https://github.com/rstudio/cheatsheets/raw/master/translations/french/data-visualization_fr.pdf){.uri}

Par exemple :

```{r}
require(ggrepel)
ggplot(manu) +
  aes(x = tx_d2, y = tx_d1, size = N_cit, label=row.names(manu)) +
    geom_point(colour = "#0c4c8a") +
  geom_text_repel() +
  xlab("Tx de dech sans les syllabes") +
  ylab("Tx de dech g?n?ral") +
  geom_vline(xintercept = 26, linetype="dashed", color = "blue")+
  geom_vline(xintercept = 44, linetype="dashed", color = "blue")+
  geom_vline(xintercept = 64, linetype="dashed", color = "blue")+
  geom_vline(xintercept = 92, linetype="dashed", color = "blue")+
  theme_minimal()
```

Affiche les noms des individus (avec label=row.names(manu)), les répartis pour qu'ils ne se chevauchent pas (avec geom_text_repel()), et fait apparaitre des ligne verticales sur les valeurs 26, 44, 64 et 92 des x (avec geom_vline(...)).

Il y a enormément d'options que l'on peut ajouter.

Par exemple : geom_hbar(yintercept = .., ...) : dessine une ligne horizontale

geom_point(position = "jitter") : modifie légèrement les coordonnées des points pour les séparer.

alpha = 0.2 dans le géom_point() permet de gerer la transparence des points

geom_smooth(method = "lm") donne la droite de régression

Pour ajouter des elements (equation de droite, tests stat, etc.) : 
https://delladata.fr/afficher-equation-droite-plot-ggplot2/

## Autres cas de figure ...

### Faire un graphique avec plusieurs courbes de densité pour comparer la distribution d'une variable numérique selon les modalités d'une variable catégorisée

On fait un graph de "density" sur la variable numérique, et on place la variable catégorisée en "color"

```{r}
ggplot(DATA) +
  aes(x = VarNum) +
  geom_density(aes (color = VarCat), size = 0.5) +
  ggtitle("Titre du graphique") +
  xlab("VarNum") +
  ylab("Densité")
```

*(l'option "size" peut être utile pour rendre le graphique + ou - lisible / joli en gérant l'épaisseur de la courbe)*

On peut afficher les moyennes des groupes sur le graphique : on commence par créé un tableau contenenant les moyennes de la variable numérique pour les modalités de la variables catégorielle ...

```{r}
moy <- DATA %>% 
  group_by(VarCat) %>%
  summarise(grp.mean = mean(VarNum, na.rm=T))
```

... et on refait le graphique en demandant des "vlines" correspondant aux moyennes que l'on vient de calculer :

```{r}
ggplot(DATA) +
  aes(x = VarNum) +
  geom_density(aes (color = VarCat)) +
  geom_vline(aes(xintercept = grp.mean, color = VarCat), data = moy, linetype = "dashed") +
  ggtitle("Titre du graphique") +
  xlab("VarNum") +
  ylab("Densité")
```

Si on remplace "color" par "fill", les aires sous les courbes seront colorées (adapter les alphas pour gérer la transparence)

```{r}
ggplot(LECT1) +
  aes(x = SCcompE) +
  geom_density(aes (fill = Cohérence.methode, alpha = 0.4)) +
  geom_vline(aes(xintercept = grp.mean, color = Cohérence.methode), data = mu, linetype = "dashed") +
  ggtitle("Score Comp écrite") +
  xlab("Score") +
  ylab("Densité")
```

# **ANALYSE GEOMETRIQUE DES DONNEES**

![](images/Q7HIP.gif){width="613"}

Possibilité d'utiliser *factoshiny* qui permet de calibrer l'analyse factorielle et d'extraire le code.

```{r}
library(Factoshiny)
Factoshiny(DATA)
```

## AFC

On commence par construire le tableau croisé sur lequel on veut faire porter l'AFC :

```{r}
t <- table(Var1 , Var2)
```

Puis on construit l'AFC :

```{r}
AFC <- CA(t)
```

Et on l'explor avec explor :

```{r}
explor(AFC)
```

## ACP

### Réduire le corpus pour n'avoir que les variables interessantes

```{r}
DAF <- DATA[, c("Var1", "Var2", "Var3", ... , "Varn")]
```

### Construire ACP avec *factominer*

```{r}
ACP<-PCA(DATA,quali.sup=c(1,7), quanti.sup=c(6),graph=FALSE)
```

(On précise les colonnes qui doivent être considérées comme illustratives, on peut faire de même pour les individus)

### Construire ACP avec *ade4*

```{r}
ACP <- dudi.pca(DATA)
```

## ACM

### Construire ACM avec factominer

```{r}

ACM<-MCA(DATA,quanti.sup=c(5,6,...,n),graph=FALSE)
```

### Explorer les résultats avec explor

```{r}
library(explor)
explor(ACM)
```

## CAH

<http://larmarange.github.io/analyse-R/classification-ascendante-hierarchique.html>

Package qui accélère la construction de la classification :

```{r}
library(fastcluster)
```

Package d'analyse géométrique des données qui permet aussi de construire des matrices de distances :

```{r}
library(ade4)
```

Package utile pour calculer des distances de gower :

```{r}
library(cluster)
```

### Avant tout, il faut construire une matrice des distances

A partir des axes d'une acm ou d'une acp :

```{r}
MatDist <- dist.dudi(acm)
```

A partir d'un jeu de données (pour baser la matrice sur une distance de gower) :

```{r}
Mat.dist <- daisy(DONNEES, metric = "gower")
```

### Construction de la classification :

```{r}
clust <- hclust(MATRICE_DISTANCES, method = "ward.D2")
```

l'argument 'method' peut prendre des termes variés selon la methode de classification que l'on veut utiliser (ici méthode de Ward appliquée au carré des distances)

### Affichage du dendogramme :

```{r}
plot(clust, labels = F)
```

ou, en un peu plus joli :

```{r}
library(ggdendro)
ggdendrogram(clust, labels = FALSE)
```

### Découper le dendogramme :

On commence par représenter l'évolution de l'inertie selon le nombre de classes pour trouver d'eventuels paliers de façon visuelle :

```{r}
inertie <- sort(clust$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
```

En général, ça suffit à mettre en avant les paliers pour la coupures. On peut même les représenter graphiquement avec des repères sur le graphique de l'inertie et sur le dendogramme :

```{r}
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
points(c(2, 7), inertie[c(2, 7)], col = c("green3", "red3"), cex = 2, lwd = 3)

plot(clust, labels = FALSE, main = "Partition en 2 ou 7 classes", xlab = "", ylab = "", sub = "", axes = FALSE, hang = -1)
rect.hclust(clust, 2, border = "green3")
rect.hclust(clust, 7, border = "red3")
```

Si ce n'est pas trés clair visuellement, ou pour avoir une confirmation, on peut utiliser :

```{r}
library(JLutils) # Si le package n'est pas mis à jour, on utilisera l'alternative de J. Larmarange :
source(url("https://raw.githubusercontent.com/larmarange/JLutils/master/R/clustering.R"))
best.cutree(clust)
```

On peut aller plus loin dans l'analyse de la solidité des classes avec :

```{r}
library(WeightedCluster)
plot(as.clustrange(clust, dist))
```

### Couper l'arbre pour construire la typologie

```{r}
typo <- cutree(clust, 5)
```

(si on veut 5 classes)

On peut créer une variable typologie dans la base de données initiale :

```{r}
DATA$typo <- cutree(clust, 5)
```

### Caractériser la typologie

```{r}
library(gtsummary)
DATA %>% tbl_summary(by=typo)
```

sort la statistique descriptive permettant de caractériser les classes

```{r}
library(GGally)
DATA$typo <- factor(DATA$typo)
ggtable(
  DATA,
  columnsX = "typo",
  columnsY = names(DATA)[1:9],
  cells = "col.prop",
  fill = "std.resid"
) +
  labs(fill = "Résidus standardizés du Chi²") +
  theme(legend.position = "bottom")
```

(Penser à ajuster le [1:9] selon le nombre de colonnes du tableau de données) Sort le tableau des résidus du khi2 pour bien repérer les différences

# **MODELISATION / REGRESSION**

<http://pbil.univ-lyon1.fr/members/fpicard/franckpicard_fichiers/pdf/siab5.pdf>

## Anova sur un facteur

<https://delladata.fr/anova-a-un-facteur-partie-1/>

### Construction du modèle :

```{r}
lm(VarNum ~ VarCat, data = DATA)
```

ou

```{r}
aov(VarNum ~ VarCat, data = DATA)
```

### Visualisation des résultats :

```{r}
library(car)
Anova(MODEL)
```

ou (si on a construit le modele avec 'aov')

```{r}
summary(MODEL)
```

### Vérification des hypothèses

#### Indépendance des résidus

```{r}
durbinWatsonTest(MODEL)
```

#### Normalité des individus

```{r}
shapiro.test(residuals(MODEL))
```

#### Homogénéité des variances

```{r}
bartlett.test(residuals(MODEL)~DATA$VarCat)
```

#### 4 Graphiques de base pour illustrer tout ça

```{r}
par(mfrow=c(2,2))
plot(lm1)
```

### Comparaison des moyennes pour voir les variations

#### Comparaison à un "cas témoin" (qui est la référence du modele)

```{r}
library(multcomp)
compDunnett <- glht(MODEL, linfct=mcp(VarCat="Dunnett"))
summary(compDunnett)
plot(compDunnett)
# Si besoin, on peut ajuster les paramètres de l'affichage avec par(mar=c(3,18,3,3))
```

#### Comparaison de toutes les modalités ensemble

```{r}
compTukey <- glht(MODEL, linfct=mcp(VarCat="Tukey"))
summary(compTukey)
plot(compTukey)
# Si besoin, on peut ajuster les paramètres de l'affichage avec par(mar=c(3,18,3,3))
```

A partir de ces comparaisons, on peut générer des groupes de modalités en attribuant un système de lettre aux modalités. Si 2 modalités partagent une même lettre, il n'y a pas d'écart significatif entre elles.

```{r}
tuk.cld <- cld(compTukey)
tuk.cld
```

Ce système de lettres peut ensuite être indexé et être réutilisé dans un graphique par exemple.

```{r}
letters <- tuk.cld$mcletters$Letters
myletters_df <- data.frame(VarCat=levels(Data$VarCat),letters=letters)
myletters_df
ggplot(DATA, aes(x=VarCat, y=VarNum, colour=VarCat, fill=VarCat))+
    geom_boxplot(outlier.alpha = 0, alpha=0.25)+
    geom_jitter(width=0.25)+  
    stat_summary(fun.y=mean, colour="black", geom="point", shape=18, size=3) +
    theme_classic()+
    theme(legend.position="none")+
    theme(axis.text.x = element_text(angle=30, hjust=1, vjust=1))+
    geom_text(data = myletters_df, aes(label = letters, y = 30 ), colour="black", size=5)
```

## Regression linéaire multiple

<https://delladata.fr/tutoriel-regression-lineaire-multiple-r/>

```{r}
library(forestmodel)
mod <- lm(A06X_SCsegm ~ Lectores_ManuON + Lectores_FichON + Lectores_WebON + Lectores_LogiON, data=cp)
summary(mod)
drop1(mod, .~. , test ="F")
hist(resid(mod))
forest_model(mod)
```

## Regression logistique

<https://larmarange.github.io/analyse-R/regression-logistique.html>

Pour faire des tableaux d'écarts bruts et nets => <https://thovenet.github.io/tab-ec.html>


# **ANALYSE TEXTUELLE ET LEXICALE**

[![](images/bouton-carr%C3%A9-bleu-de-cliquez-ici-89657551.jpg){width="222"}](https://thovenet.github.io/Analyse-textuelle.html)

# **AIDE-MEMOIRES ET REFERENCES**

## Utilisation de R-studio

<https://raw.githubusercontent.com/rstudio/cheatsheets/master/translations/french/rstudio-ide_fr.pdf>

## Quelques bases de langage

<https://raw.githubusercontent.com/rstudio/cheatsheets/master/base-r.pdf>

## Utlisation Rmarkdown

<https://github.com/rstudio/cheatsheets/raw/master/rmarkdown.pdf>

## Aide-mémoire sur les fonctions r souvent utilisées :

<https://cran.r-project.org/doc/contrib/Kauffmann_aide_memoire_R.pdf>

## Guide d'utilisation de R Markdown :

<https://bookdown.org/yihui/rmarkdown/>

## Blogs / sites de conseils :

-   <https://larmarange.github.io/analyse-R/>

-   <https://delladata.fr/>

-   <http://perso.ens-lyon.fr/lise.vaudor/>

-   <https://rzine.fr/>

-   <https://mtes-mct.github.io/parcours_r_socle_introduction/>

## Cours en ligne SHS et big data :

<https://www.css.cnrs.fr/strategies-numeriques/>
